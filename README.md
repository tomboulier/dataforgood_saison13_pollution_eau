# Rendre Visible la Pollution de l'Eau Potable üíß

## Contexte du Projet

Ce projet, d√©velopp√© par des b√©n√©voles de [Data For Good](https://www.dataforgood.fr/) lors de la saison 13, vise √† cr√©er une carte interactive pour [G√©n√©rations Futures](https://www.generations-futures.fr/).

L'objectif est de consolider, analyser et cartographier les donn√©es sur la qualit√© de l'eau potable en France √† partir de sources de donn√©es ouvertes.

## Structure du Projet

- `pipelines/` : Consolidation et pr√©paration des donn√©es
- `analytics/` : Analyse des donn√©es
- `webapp/` : D√©veloppement du site web interactif

## Installation

### Data Pipelines

- [Installation de Python](#installation-de-python)

Ce projet utilise [uv](https://docs.astral.sh/uv/) pour la gestion des d√©pendances Python. Il est pr√©r√©quis pour l'installation de ce projet.

Installation sur Windows

```bash
powershell -c "irm https://astral.sh/uv/install.ps1 | iex"
```

Installation sur Mac ou linux

```bash
curl -LsSf https://astral.sh/uv/install.sh | sh
```

Une fois install√©, il suffit de lancer la commande suivante pour installer la version de Python ad√©quate, cr√©er un environnement virtuel et installer les d√©pendances du projet.

```bash
uv sync
```

#### VSCode

A l'usage, si vous utilisez VSCode, l'environnement virtuel sera automatiquement activ√© lorsque vous ouvrirez le projet. Sinon, il suffit de l'activer manuellement avec la commande suivante :

```bash
source .venv/bin/activate
```

Ou alors, utilisez la commande `uv run ...` (au lieu de `python ...`) pour lancer un script Python. Par exemple:

```bash
uv run pipelines/run.py run build_database
```

#### Pycharm

Allez dans settings, python interpreter, add interpreter, puis selectionnez existing venv et allez chercher le path du python executable dans .venv (.venv/Scripts/Python.exe pour windows)

#### Terminal

utilisez les commandes `uv run` pour lancer un script Python depuis votre terminal

- [Installation de Node.js](#installation-de-nodejs) (pour le d√©veloppement du site web et pour l'usage de Evidence)

Pour le d√©veloppement du site web et pour l'usage de [Evidence](https://evidence.dev/), il est n√©cessaire d'installer Node.js. Pour cela, il suffit de suivre les instructions sur le [site officiel](https://nodejs.org/).

Pour installer les d√©pendances du site web, il suffit de lancer les commandes suivantes :

```bash
cd webapp
npm install
```

## Data Processing

### Package installation

Tout le code dans pipelines sera install√© en tant que package python automatiquement √† chaque uv_sync

### Comment construire la database

Une fois l'environnement python setup avec uv, vous pouvez lancer data_pipeline/run.py pour remplir la database
Il suffit de lancer

```bash
uv run pipelines/run.py run build_database
```

### Comment t√©l√©charger la database depuis S3

Des versions de dev et de production de la db sont √† disposition sur le storage object.
Il faut bien configurer ses credentials et son env via le fichier .env.
Ensuite il suffit de lancer

```bash
uv run pipelines/run.py run download_database
```

### Connection a Scaleway via boto3 pour stockage cloud

Un utils a √©t√© cr√©√© dans [storage_client.py](pipelines%2Futils%2Fstorage_client.py) pour faciliter la connection au S3 h√©berg√© sur Scaleway.

Il faut cr√©er un fichier .env dans le dossier pipelines/config avec les secrets ci dessous dedans pour que la connection fonctionne.

```text
SCW_ACCESS_KEY={ACCESS_KEY}
SCW_SECRET_KEY={SECRET_KEY}
```

Vous trouverez un example avec le fichier [.env.example](pipelines%2Fconfig%2F.env.example)

> ‚ö† **Attention:** Ne jamais commir les access key et secret key.

Un vaultwarden va √™tre setup pour r√©cup√©rer les secrets pour les personnes qui en ont besoin

Le notebook [test_storage_utils.ipynb](pipelines%2Fnotebooks%2Ftest_storage_utils.ipynb) montre un example d'utilisation de l'utils pour charger et lire des csv sur le bucket S3 du projet

### Data analysis

Les analyses ce font via jupyter notebook

```bash
uv run jupyter notebook
```

## Pre Commit

Lancer la commande suivante pour s'assurer que le code satisfait bien tous les pre commit avant de cr√©er votre pull request

```ba*sh
pre-commit run --all-files
```

## How to contribute
Pour contribuer, il est recommand√© d'utiliser un fork du projet. Cela permet d'√©viter la gestion des demandes d'acc√®s au d√©p√¥t principal.

* Dans un premier temps, cliquez sur Fork pour r√©cup√©rer le projet dans votre espace GitHub.
* Cr√©ez votre branche de travail √† partir de la branche main, en respectant la nomenclature suivante :
  * feature/nom_de_la_feature pour une nouvelle fonctionnalit√©
  * hotfix/nom_du_hotfix pour une correction rapide
* Poussez votre code vers votre d√©p√¥t distant.
* Cr√©ez une pull request en sp√©cifiant :
  * Base repository : dataforgood/13_pollution_eau/main
  * Head repository : YourGithubAccount/13_pollution_eau/your_branch
* Pour faciliter la revue de la pull request :
  * Liez la pull request √† un ticket NocoDB en ajoutant le lien du ticket dans la description.
  * R√©digez une description d√©taill√©e de la pull request afin de fournir un maximum d‚Äôinformations sur les modifications apport√©es.
